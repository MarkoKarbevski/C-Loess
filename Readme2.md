# CLR
Localised linear regression python code

Loess is a well-established machine learning algorithm. Unfortunately, its complexity for predicting <img src="/tex/63bb9849783d01d91403bc9a5fea12a2.svg?invert_in_darkmode&sanitize=true" align=middle width=9.075367949999992pt height=22.831056599999986pt/> outputs out of a dataset of <img src="/tex/55a049b8f161ae7cfeb0197d75aff967.svg?invert_in_darkmode&sanitize=true" align=middle width=9.86687624999999pt height=14.15524440000002pt/> samples with dimension <img src="/tex/2ec6e630f199f589a2402fdf3e0289d5.svg?invert_in_darkmode&sanitize=true" align=middle width=8.270567249999992pt height=14.15524440000002pt/> is <img src="/tex/d8fe06ebc8b29036cbb5a50ccd417a5d.svg?invert_in_darkmode&sanitize=true" align=middle width=108.88977494999999pt height=26.76175259999998pt/> in case of the exact computation (using matrix inverses), or <img src="/tex/c09acd2fb583f905bea6ff7443777c45.svg?invert_in_darkmode&sanitize=true" align=middle width=58.92976154999999pt height=24.65753399999998pt/> for a (stochastic) gradient descent, where <img src="/tex/4f4f4e395762a3af4575de74c019ebb5.svg?invert_in_darkmode&sanitize=true" align=middle width=5.936097749999991pt height=20.221802699999984pt/> is the number of steps needed for the training. 

